{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation\n",
    "\n",
    "+ Most commonly used in natural language processing\n",
    "+ Sometimes as an end in and of itself\n",
    "+ Sometimes as a variable reduction technique\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Example of LDA in NLP\n",
    "\n",
    "Stolen from: http://scikit-learn.org/stable/auto_examples/applications/topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-topics-extraction-with-nmf-lda-py\n",
    "\n",
    "+ Authors: \n",
    "    + Olivier Grisel <olivier.grisel@ensta.org>\n",
    "    + Lars Buitinck\n",
    "    + Chyi-Kwei Yau <chyikwei.yau@gmail.com>\n",
    "+ License: BSD 3 clause"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 53,
=======
   "execution_count": 1,
>>>>>>> 9717ebe70e3c030cc6e4569f2c1b9649a7a86239
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from time import time\n",
<<<<<<< HEAD
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
=======
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
>>>>>>> 9717ebe70e3c030cc6e4569f2c1b9649a7a86239
    "from sklearn.datasets import fetch_20newsgroups\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This code defines a custom function that we'll use later"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 54,
=======
   "execution_count": 2,
>>>>>>> 9717ebe70e3c030cc6e4569f2c1b9649a7a86239
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "n_samples = 2000  #posts / articles\n",
    "n_features = 1000 # Words\n",
    "n_topics = 10 \n",
=======
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_topics = 10\n",
>>>>>>> 9717ebe70e3c030cc6e4569f2c1b9649a7a86239
    "n_top_words = 20\n",
    "\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This code loads the dataset"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 55,
=======
   "execution_count": 11,
>>>>>>> 9717ebe70e3c030cc6e4569f2c1b9649a7a86239
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
<<<<<<< HEAD
      "done in 3.242s.\n"
=======
      "done in 1.957s.\n"
>>>>>>> 9717ebe70e3c030cc6e4569f2c1b9649a7a86239
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n",
    "# to filter out useless terms early on: the posts are stripped of headers,\n",
    "# footers and quoted replies, and common English words, words occurring in\n",
    "# only one document or in at least 95% of the documents are removed.\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
<<<<<<< HEAD
=======
    "\n",
>>>>>>> 9717ebe70e3c030cc6e4569f2c1b9649a7a86239
    "data_samples = dataset.data[:n_samples]\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 56,
=======
   "execution_count": 15,
>>>>>>> 9717ebe70e3c030cc6e4569f2c1b9649a7a86239
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"\\n\\n\\n\\n\\tI'd like to see this info as well.  As for wavelength, I think\\nyou're primarily going to find two - 880 nM +/- a bit, and/or 950 nM\\n+/- a bit.  Usually it is about 10 nM either way.  The two most common\\nI have seen were 880 and 950 but I have also heard of 890 and 940.\\nI'm not sure that the 10 nM one way or another will make a great deal of\\ndifference.\\n\\n\\tAnother suggestion - find a brand of TV that uses an IR remote,\\nand go look at the SAMS photofact for it.  You can often find some very\\ndetailed schematics and parts list for not only the receiver but the\\ntransmitter as well, including carrier freq. specs. and tone decoding\\nspecs. if the system uses that.\""
      ]
     },
<<<<<<< HEAD
     "execution_count": 56,
=======
     "execution_count": 15,
>>>>>>> 9717ebe70e3c030cc6e4569f2c1b9649a7a86239
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['data'][20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
<<<<<<< HEAD
    "USE term frequancy TF (raw term count) features for LDA"
=======
    "## Use tf (raw term count) features for LDA."
>>>>>>> 9717ebe70e3c030cc6e4569f2c1b9649a7a86239
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 84,
=======
   "execution_count": 16,
>>>>>>> 9717ebe70e3c030cc6e4569f2c1b9649a7a86239
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
<<<<<<< HEAD
      "done in 0.646s.\n"
=======
      "done in 0.394s.\n"
>>>>>>> 9717ebe70e3c030cc6e4569f2c1b9649a7a86239
     ]
    }
   ],
   "source": [
    "# Use tf (raw term count) features for LDA.\n",
<<<<<<< HEAD
    "\n",
    "#DF as document frequancy\n",
=======
>>>>>>> 9717ebe70e3c030cc6e4569f2c1b9649a7a86239
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 58,
=======
   "execution_count": 20,
>>>>>>> 9717ebe70e3c030cc6e4569f2c1b9649a7a86239
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA models with tf features, n_samples=2000 and n_features=1000...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
<<<<<<< HEAD
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=50,\n",
=======
    "lda = LatentDirichletAllocation(n_topics=10, max_iter=10,\n",
>>>>>>> 9717ebe70e3c030cc6e4569f2c1b9649a7a86239
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 68,
=======
   "execution_count": 21,
>>>>>>> 9717ebe70e3c030cc6e4569f2c1b9649a7a86239
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "done in 50.724s.\n"
=======
      "done in 4.546s.\n"
>>>>>>> 9717ebe70e3c030cc6e4569f2c1b9649a7a86239
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
<<<<<<< HEAD
    "X = lda.fit_transform(tf)\n",
=======
    "lda.fit(tf)\n",
>>>>>>> 9717ebe70e3c030cc6e4569f2c1b9649a7a86239
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00344879,  0.20662193,  0.22790615,  0.00344876,  0.00344859,\n",
       "        0.54133022,  0.00344853,  0.00344855,  0.00344935,  0.00344914])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
=======
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
>>>>>>> 9717ebe70e3c030cc6e4569f2c1b9649a7a86239
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "print(lda.print_topics(num_topics=10, num_words=3))"
=======
    "#print(lda.print_topics(num_topics=10, num_words=3))"
>>>>>>> 9717ebe70e3c030cc6e4569f2c1b9649a7a86239
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model:\n",
      "Topic #0:\n",
<<<<<<< HEAD
      "edu com mail send graphics ftp pub available contact version list ca faq program university cs machines server sun file\n",
      "Topic #1:\n",
      "just don like think know good people way ve make right going want use really ll time say sure doesn\n",
      "Topic #2:\n",
      "think book new president atheism game pittsburgh history jobs media like subject play time magi people mark just read radio\n",
      "Topic #3:\n",
      "drive windows disk use thanks card drives using hard does software scsi problem file 16 help controller pc time need\n",
      "Topic #4:\n",
      "hiv health aids april research disease care information medical 1993 national new children study said service university test number drug\n",
      "Topic #5:\n",
      "god people does jesus law bible say israel life just believe don church fact think way good know time did\n",
      "Topic #6:\n",
      "10 55 11 15 12 game 18 team 20 19 13 period 17 23 16 25 21 24 22 14\n",
      "Topic #7:\n",
      "car new year bike price cars good engine oil insurance better just used speed 00 like 000 sell years 100\n",
      "Topic #8:\n",
      "people said didn went did know time took just came like dead started come old armenians children happened home told\n",
      "Topic #9:\n",
      "key space government use public law encryption section clipper chip keys earth security data moon probe military enforcement used states\n",
=======
      "edu com mail send graphics\n",
      "Topic #1:\n",
      "don like just know think\n",
      "Topic #2:\n",
      "think christian atheism book pittsburgh\n",
      "Topic #3:\n",
      "drive windows disk thanks use\n",
      "Topic #4:\n",
      "hiv health aids april disease\n",
      "Topic #5:\n",
      "god people does just jesus\n",
      "Topic #6:\n",
      "55 10 11 game 15\n",
      "Topic #7:\n",
      "car year new cars bike\n",
      "Topic #8:\n",
      "people said didn did know\n",
      "Topic #9:\n",
      "key space government public law\n",
>>>>>>> 9717ebe70e3c030cc6e4569f2c1b9649a7a86239
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
<<<<<<< HEAD
    "print_top_words(lda, tf_feature_names, n_top_words)"
=======
    "print_top_words(lda, tf_feature_names, 5)"
>>>>>>> 9717ebe70e3c030cc6e4569f2c1b9649a7a86239
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In class assignment\n",
    "\n",
    "+ load in the training set (done for you below)\n",
    "+ re-run LDA and use topics as input for model\n",
    "+ Predict categories using some multinomial classifier "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 3.314s.\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
>>>>>>> 9717ebe70e3c030cc6e4569f2c1b9649a7a86239
   "source": [
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'), \n",
    "                            subset=\"train\")\n",
    "\n",
    "data = dataset.data\n",
    "\n",
<<<<<<< HEAD
    "y = dataset.target[:n_samples]\n",
=======
    "y = dataset.target\n",
>>>>>>> 9717ebe70e3c030cc6e4569f2c1b9649a7a86239
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'LatentDirichletAllocation' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-ec555b40e03b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                                 stop_words='english')\n\u001b[1;32m      6\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"done in %0.3fs.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/matanefron/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 839\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/matanefron/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    758\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_int_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0mindptr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 760\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    761\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'LatentDirichletAllocation' object is not iterable"
     ]
    }
   ],
   "source": [
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(lda)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
=======
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use tf (raw term count) features for LDA.\n",
>>>>>>> 9717ebe70e3c030cc6e4569f2c1b9649a7a86239
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 81,
=======
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
>>>>>>> 9717ebe70e3c030cc6e4569f2c1b9649a7a86239
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=20, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators = 20)\n",
    "    \n",
    "model.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17,  0, 17, ...,  3, 15,  9])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.3 ,  0.  ,  0.  , ...,  0.4 ,  0.05,  0.  ],\n",
       "       [ 0.65,  0.  ,  0.  , ...,  0.  ,  0.05,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  , ...,  0.6 ,  0.  ,  0.05],\n",
       "       ..., \n",
       "       [ 0.  ,  0.  ,  0.  , ...,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.05,  0.  ,  0.  , ...,  0.05,  0.  ,  0.1 ],\n",
       "       [ 0.  ,  0.05,  0.  , ...,  0.  ,  0.  ,  0.  ]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(X)\n"
   ]
  },
  {
=======
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-bb44b819a0e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clf' is not defined"
     ]
    }
   ],
   "source": [
    "X_test = tf_vectorizer.transform(dataset.data)\n",
    "X_test = lda.transform(X_test)\n",
    "\n",
    "pred = clf.predict(X_test)\n",
    "metrics.f1_score(dataset.target, pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
>>>>>>> 9717ebe70e3c030cc6e4569f2c1b9649a7a86239
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In class assignment:\n",
    "\n",
    "+ I'll divide you into 3 segments\n",
    "+ Each segment generates 100 sentences on the *same topic*\n",
    "+ Save as a JSON and send to me\n",
    "+ We'll run them through LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CLASSLIST = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
<<<<<<< HEAD
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
=======
>>>>>>> 9717ebe70e3c030cc6e4569f2c1b9649a7a86239
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
<<<<<<< HEAD
   "version": "2.7.13"
=======
   "version": "2.7.11"
>>>>>>> 9717ebe70e3c030cc6e4569f2c1b9649a7a86239
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
